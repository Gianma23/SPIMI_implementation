{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "uaEpy78A6TVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ir_datasets -q\n",
        "!pip install nltk -q\n",
        "!pip install ir_measures -q\n",
        "!pip install PyStemmer -q\n",
        "!pip install pandas -q\n",
        "!pip install zstandard -q\n",
        "!pip install python-terrier -q\n",
        "!pip install --upgrade gdown -q"
      ],
      "metadata": {
        "id": "Y3hyYEQb6XMK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ir_datasets\n",
        "import ir_measures\n",
        "from ir_measures import *\n",
        "import random\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "import time\n",
        "from collections import Counter, defaultdict\n",
        "from tqdm.auto import tqdm\n",
        "import gzip\n",
        "import zstandard as zstd\n",
        "import pickle\n",
        "import os\n",
        "import heapq\n",
        "import math\n",
        "import pyterrier as pt\n",
        "from google.colab import drive\n",
        "import os\n",
        "import shutil"
      ],
      "metadata": {
        "id": "Pkr3Z4ZwXgR9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def profile(f):\n",
        "    def f_timer(*args, **kwargs):\n",
        "        start = time.time()\n",
        "        result = f(*args, **kwargs)\n",
        "        end = time.time()\n",
        "        ms = (end - start) * 1000\n",
        "        print(f\"{f.__name__} ({ms:.3f} ms)\")\n",
        "        return result\n",
        "    return f_timer"
      ],
      "metadata": {
        "id": "D6IrpgQFGaEH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ir_datasets.load(\"msmarco-passage\")\n",
        "\n",
        "# Mount Google Drive to access required files\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# # URL of the Google Drive repository containing the project files\n",
        "# repository = \"YOUR_REPOSITORY_URL\"\n",
        "# repository_name = \"ir-project-files\"\n",
        "\n",
        "# # Download the specified folder from the repository\n",
        "# !gdown --folder $repository\n",
        "\n",
        "# # Copy the downloaded files from the repository folder to /content/\n",
        "# # This ensures the files are easily accessible during execution\n",
        "# for item in os.listdir(repository_name):\n",
        "#   s = os.path.join(repository_name, item)\n",
        "#   d = os.path.join('/content/', item)\n",
        "#   if os.path.isfile(s):             # Check if the item is a file before copying\n",
        "#     shutil.copy2(s, d)\n",
        "\n",
        "# # Remove the downloaded repository folder to free up space\n",
        "# shutil.rmtree(repository_name)"
      ],
      "metadata": {
        "id": "aKFtMhztXhiz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cca6737-2b19-4fa1-9aab-5bc60c898d46"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "UU7jG_EjIOir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import lru_cache\n",
        "import Stemmer\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "\n",
        "# ------- Pre Initialization -------\n",
        "# Compile reusable resources and pre-load common datasets for efficiency\n",
        "# 1. Regular expression for removing unnecessary dots in acronyms\n",
        "# 2. Translation table for stripping punctuation\n",
        "# 3. Set of English stopwords for filtering irrelevant tokens\n",
        "# 4. Initialize a stemming tool for word normalization\n",
        "\n",
        "ACRONYM_REGEX = re.compile(r\"(?<!\\w)\\.(?!\\d)\")                  # Matches dots not part of decimal numbers\n",
        "PUNCTUATION_TRANS = str.maketrans(\"\", \"\", string.punctuation)   # Removes punctuation\n",
        "STOPWORDS = set(nltk.corpus.stopwords.words('english'))         # Load English stopwords\n",
        "STEMMER = Stemmer.Stemmer('english')                            # Initialize an English stemmer\n",
        "# ----------------------------------\n",
        "\n",
        "def preprocess(s):\n",
        "    \"\"\"\n",
        "    Preprocesses an input string for text analysis tasks such as indexing or querying.\n",
        "\n",
        "    Args:\n",
        "        s (str): The input string to preprocess.\n",
        "\n",
        "    Returns:\n",
        "        list[str]: A list of processed tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    s = s.lower()\n",
        "    s = s.replace(\"&\", \" and \")\n",
        "    s = s.translate(str.maketrans(\"‘’´“”–-\", \"'''\\\"\\\"--\"))      # Standardize quotes and dashes for uniformity\n",
        "    s = ACRONYM_REGEX.sub(\"\", s)                                # Remove unnecessary dots in acronyms (but not decimals)\n",
        "    s = s.translate(PUNCTUATION_TRANS)                          # Remove all punctuation\n",
        "    s = \" \".join(s.split())                                     # Remove extra spaces and strip leading/trailing spaces\n",
        "\n",
        "    tokens = s.split()\n",
        "    tokens = [t for t in tokens if t not in STOPWORDS]          # Filter out stopwords\n",
        "    tokens = STEMMER.stemWords(tokens)                          # Apply stemming to normalize word forms\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "kjnoPt6_IDEU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the inverted index"
      ],
      "metadata": {
        "id": "rRT83CcEMDsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@profile\n",
        "def build_index(dataset):\n",
        "    \"\"\"\n",
        "    Constructs an inverted index from a dataset.\n",
        "\n",
        "    The function processes documents to build the following components:\n",
        "    1. Lexicon: Maps terms to term IDs and tracks document frequency (DF) and term frequency (TF).\n",
        "    2. Inverted Index: Maps term IDs to lists of document IDs and term frequencies.\n",
        "    3. Document Index: A list of document IDs and their corresponding document lengths.\n",
        "    4. Index Statistics: A dictionary summarizing the index statistics.\n",
        "\n",
        "    Args:\n",
        "        dataset: The dataset to index.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - lexicon (dict): Maps terms to [term ID, document frequency, term frequency].\n",
        "            - inverted_index (dict): Contains:\n",
        "                - 'docids' (dict): Maps term IDs to lists of document IDs.\n",
        "                - 'freqs' (dict): Maps term IDs to lists of term frequencies in the documents.\n",
        "            - document_index (list): A list of tuples (document ID, document length).\n",
        "            - stats (dict): Contains:\n",
        "                - 'num_docs': Total number of documents indexed.\n",
        "                - 'num_terms': Total number of unique terms.\n",
        "                - 'num_tokens': Total number of tokens across all documents.\n",
        "    \"\"\"\n",
        "\n",
        "    lexicon = {}                # Maps terms to [term ID, document frequency, term frequency]\n",
        "    doc_index = []              # Stores document IDs and their lengths\n",
        "    inv_d, inv_f = {}, {}       # Inverted index components: doc IDs and term frequencies\n",
        "    termid = 0                  # Counter for assigning unique term IDs\n",
        "\n",
        "    num_docs = 0                # Number of documents processed\n",
        "    total_dl = 0                # Total length of the documents (in tokens)\n",
        "\n",
        "    # Iterate over documents in the dataset\n",
        "    for docid, doc in tqdm(enumerate(dataset.docs_iter()), desc='Indexing', total=dataset.docs_count()):\n",
        "        tokens = preprocess(doc.text)               # Preprocess document text into tokens\n",
        "        token_tf = Counter(tokens)                  # Count term frequencies in the document\n",
        "\n",
        "        # Populate the lexicon and inverted index\n",
        "        for token, tf in token_tf.items():          # Assign a new term ID if the token is not in the lexicon\n",
        "            if token not in lexicon:\n",
        "                lexicon[token] = [termid, 0, 0]\n",
        "                inv_d[termid], inv_f[termid] =  [], []\n",
        "                termid += 1\n",
        "\n",
        "            token_id = lexicon[token][0]            # Get the term ID\n",
        "            inv_d[token_id].append(docid)\n",
        "            inv_f[token_id].append(tf)\n",
        "            lexicon[token][1] += 1                  # Increment document frequency for the term\n",
        "            lexicon[token][2] += tf                 # Increment total term frequency\n",
        "\n",
        "        # Update document index and statistics\n",
        "        doclen = len(tokens)\n",
        "        doc_index.append((str(doc.doc_id), doclen)) # Add document ID and length to the index\n",
        "        total_dl += doclen\n",
        "        num_docs += 1\n",
        "\n",
        "    # Build index statistics\n",
        "    stats = {\n",
        "        'num_docs': 1 + docid,                      # Total number of documents indexed\n",
        "        'num_terms': len(lexicon),                  # Total number of unique terms\n",
        "        'num_tokens': total_dl,                     # Total number of tokens across all documents\n",
        "        \"avg_length\": total_dl / (1 + docid)\n",
        "    }\n",
        "\n",
        "    return lexicon, {'docids': inv_d, 'freqs': inv_f}, doc_index, stats"
      ],
      "metadata": {
        "id": "MGXAioOGMKYF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nuovo codice con aggiornamenti parziali:\n"
      ],
      "metadata": {
        "id": "dAx6_K1b4lN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import zstandard as zstd\n",
        "from collections import Counter, defaultdict\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "\n",
        "CHUNK_SIZE = 5_000_000  # 5 million docs\n",
        "COMPRESSOR = zstd.ZstdCompressor(level=10)\n",
        "DECOMPRESSOR = zstd.ZstdDecompressor()\n",
        "\n",
        "def preprocess(text):\n",
        "    # Simple preprocessing (placeholder, you may customize)\n",
        "    return text.lower().split()\n",
        "\n",
        "def vb_encode_number(n):\n",
        "    bytes_ = []\n",
        "    while True:\n",
        "        bytes_.insert(0, n % 128)\n",
        "        if n < 128:\n",
        "            break\n",
        "        n //= 128\n",
        "    bytes_[-1] += 128\n",
        "    return bytes_\n",
        "\n",
        "def vb_encode_list(numbers):\n",
        "    bytestream = []\n",
        "    for n in numbers:\n",
        "        bytestream.extend(vb_encode_number(n))\n",
        "    return bytestream\n",
        "\n",
        "def delta_encode(arr):\n",
        "    if not arr:\n",
        "        return []\n",
        "    encoded = [arr[0]]\n",
        "    for i in range(1, len(arr)):\n",
        "        encoded.append(arr[i] - arr[i - 1])\n",
        "    return encoded\n",
        "\n",
        "def _save_partial_index(inv_d, inv_f, out_dir, chunk_id):\n",
        "    inv_path = os.path.join(out_dir, f\"inverted_chunk_{chunk_id}.pkl.zst\")\n",
        "    with open(inv_path, \"wb\") as f:\n",
        "        with COMPRESSOR.stream_writer(f) as writer:\n",
        "            pickle.dump({\"docids\": dict(inv_d), \"freqs\": dict(inv_f)}, writer)\n",
        "\n",
        "def build_spimi_index(dataset, out_dir=\"index_chunks\"):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    lexicon = {}\n",
        "    doc_index = []\n",
        "    termid_counter = 0\n",
        "    docid_global = 0\n",
        "    total_tokens = 0\n",
        "    chunk_id = 0\n",
        "    inv_d = defaultdict(list)\n",
        "    inv_f = defaultdict(list)\n",
        "\n",
        "    for docid, doc in tqdm(enumerate(dataset.docs_iter()), total=dataset.docs_count(), desc=\"Indexing\"):\n",
        "        tokens = preprocess(doc.text)\n",
        "        token_tf = Counter(tokens)\n",
        "\n",
        "        for token, tf in token_tf.items():\n",
        "            if token not in lexicon:\n",
        "                lexicon[token] = [termid_counter, 0, 0]\n",
        "                termid_counter += 1\n",
        "            term_id = lexicon[token][0]\n",
        "            lexicon[token][1] += 1\n",
        "            lexicon[token][2] += tf\n",
        "            inv_d[term_id].append(docid_global)\n",
        "            inv_f[term_id].append(tf)\n",
        "\n",
        "        doc_index.append((str(doc.doc_id), len(tokens)))\n",
        "        total_tokens += len(tokens)\n",
        "        docid_global += 1\n",
        "\n",
        "        if docid_global % CHUNK_SIZE == 0:\n",
        "            _save_partial_index(inv_d, inv_f, out_dir, chunk_id)\n",
        "            chunk_id += 1\n",
        "            inv_d.clear()\n",
        "            inv_f.clear()\n",
        "\n",
        "    if inv_d:\n",
        "        _save_partial_index(inv_d, inv_f, out_dir, chunk_id)\n",
        "        inv_d.clear()\n",
        "        inv_f.clear()\n",
        "\n",
        "    with open(os.path.join(out_dir, \"doc_index.pkl.zst\"), \"wb\") as f:\n",
        "        with COMPRESSOR.stream_writer(f) as writer:\n",
        "            pickle.dump(doc_index, writer)\n",
        "\n",
        "    stats = {\n",
        "        \"num_docs\": docid_global,\n",
        "        \"num_terms\": len(lexicon),\n",
        "        \"num_tokens\": total_tokens,\n",
        "        \"avg_length\": total_tokens / docid_global\n",
        "    }\n",
        "    with open(os.path.join(out_dir, \"stats.pkl.zst\"), \"wb\") as f:\n",
        "        with COMPRESSOR.stream_writer(f) as writer:\n",
        "            pickle.dump(stats, writer)\n",
        "\n",
        "    with open(os.path.join(out_dir, \"lexicon.pkl\"), \"wb\") as f:\n",
        "        pickle.dump(lexicon, f)\n",
        "\n",
        "    merge_spimi_chunks(out_dir)\n",
        "\n",
        "    return lexicon, stats\n",
        "\n",
        "def merge_spimi_chunks(input_dir, output_index_path=\"merged_index.zst\", output_lexicon_path=\"lexicon_with_offsets.pkl\"):\n",
        "    merged_docids = defaultdict(list)\n",
        "    merged_freqs = defaultdict(list)\n",
        "    chunk_files = sorted(glob(os.path.join(input_dir, \"inverted_chunk_*.pkl.zst\")))\n",
        "    for chunk_file in chunk_files:\n",
        "        with open(chunk_file, \"rb\") as f:\n",
        "            with DECOMPRESSOR.stream_reader(f) as reader:\n",
        "                chunk = pickle.load(reader)\n",
        "                for termid in chunk[\"docids\"]:\n",
        "                    merged_docids[termid].extend(chunk[\"docids\"][termid])\n",
        "                    merged_freqs[termid].extend(chunk[\"freqs\"][termid])\n",
        "\n",
        "    offset_lexicon = {}\n",
        "    with open(os.path.join(input_dir, \"lexicon.pkl\"), \"rb\") as f:\n",
        "        base_lexicon = pickle.load(f)\n",
        "\n",
        "    with open(output_index_path, \"wb\") as index_file:\n",
        "        with COMPRESSOR.stream_writer(index_file) as writer:\n",
        "            offset = 0\n",
        "            for term, (term_id, df, tf) in base_lexicon.items():\n",
        "                postings = sorted(zip(merged_docids[term_id], merged_freqs[term_id]))\n",
        "                docids_sorted, freqs_sorted = zip(*postings)\n",
        "                docids_delta = delta_encode(docids_sorted)\n",
        "                vb_docids = vb_encode_list(docids_delta)\n",
        "                vb_freqs = vb_encode_list(freqs_sorted)\n",
        "                entry = {\"docids_vb\": vb_docids, \"freqs_vb\": vb_freqs}\n",
        "                entry_bytes = pickle.dumps(entry)\n",
        "                writer.write(entry_bytes)\n",
        "                offset_lexicon[term] = (term_id, df, tf, offset, len(entry_bytes))\n",
        "                offset += len(entry_bytes)\n",
        "\n",
        "    with open(output_lexicon_path, \"wb\") as f:\n",
        "        pickle.dump(offset_lexicon, f)\n",
        "\n",
        "    return output_lexicon_path, output_index_path\n",
        "\n"
      ],
      "metadata": {
        "id": "McHpEpm24n6t"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merge_spimi_chunks(\"index_chunks\")"
      ],
      "metadata": {
        "id": "-iHQFx0x57CN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creazione indice inverso:"
      ],
      "metadata": {
        "id": "prJXxmiFM-mW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lex, inv, doc, stats = None, None, None, None               # Initialize variables for the index components\n",
        "\n",
        "files = ['lexicon.pickle.zst', 'inverted_file.pickle.zst', 'document_index.pickle.zst', 'stats.pickle.zst']\n",
        "if all(os.path.exists(file) for file in files):             # Check if all required files exist\n",
        "    print(\"All files already exist.\")\n",
        "\n",
        "    # Iterate over the list of files and their associated variable names\n",
        "    for file, var_name in zip(files, ['lex', 'inv', 'doc', 'stats']):\n",
        "        try:\n",
        "            if os.path.getsize(file) > 0:                   # Ensure the file is not empty\n",
        "                with gzip.open(file, 'rb') as f:\n",
        "                    globals()[var_name] = pickle.load(f)    # Load the file into the corresponding variable\n",
        "            else:\n",
        "                print(f\"Warning: {file} is empty.\")\n",
        "        except EOFError:\n",
        "            # If the file is corrupted or incomplete, rebuild the index\n",
        "            print(f\"Error: {file} is corrupted or incomplete. Rebuilding the index.\")\n",
        "            lex, inv, doc, stats = build_spimi_index(dataset)\n",
        "            break\n",
        "else:\n",
        "    # If any of the files do not exist, rebuild the index\n",
        "    lex, inv, doc, stats = build_spimi_index(dataset)\n",
        "\n",
        "    # Save the rebuilt index components back into the respective files\n",
        "    cctx = zstd.ZstdCompressor(level=10)\n",
        "    for data, file in zip([lex, inv, doc, stats], files):\n",
        "      with open(file, 'wb') as f:\n",
        "        with cctx.stream_writer(f) as compressor:\n",
        "          print(f\"Saving {file}...\")\n",
        "          pickle.dump(data, compressor)                                # Serialize and save the data\n"
      ],
      "metadata": {
        "id": "LELM2D1qOJCR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4c9b8fe-8adb-483d-a569-b3b9ca7bde3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Indexing: 100%|██████████| 8841823/8841823 [19:43<00:00, 7467.81it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compression"
      ],
      "metadata": {
        "id": "O_pX-O9jOwz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compression lexicon:\n",
        "\n",
        "*   delta encoding per l'array\n",
        "*   front coding per i termini\n",
        "\n"
      ],
      "metadata": {
        "id": "G2ewZNOgmQlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def front_coding_encode(terms):\n",
        "    compressed = []\n",
        "    prev = terms[0]\n",
        "    compressed.append(prev)\n",
        "    for curr in terms[1:]:\n",
        "        i = 0\n",
        "        while i < min(len(prev), len(curr)) and prev[i] == curr[i]:\n",
        "            i += 1\n",
        "        suffix = curr[i:]\n",
        "        compressed.append(f\"{i}|{suffix}\")\n",
        "        prev = curr\n",
        "    return compressed\n",
        "\n",
        "def front_coding_decode(front_coded):\n",
        "    terms = [front_coded[0]]\n",
        "    for item in front_coded[1:]:\n",
        "        prefix_len, suffix = item.split(\"|\", 1)\n",
        "        prefix_len = int(prefix_len)\n",
        "        term = terms[-1][:prefix_len] + suffix\n",
        "        terms.append(term)\n",
        "    return terms\n",
        "\n",
        "def delta_encode(arr):\n",
        "    if not arr:\n",
        "        return []\n",
        "    encoded = [arr[0]]\n",
        "    for i in range(1, len(arr)):\n",
        "        encoded.append(arr[i] - arr[i-1])\n",
        "    return encoded\n",
        "\n",
        "def delta_decode(arr):\n",
        "    if not arr:\n",
        "        return []\n",
        "    decoded = [arr[0]]\n",
        "    for i in range(1, len(arr)):\n",
        "        decoded.append(decoded[-1] + arr[i])\n",
        "    return decoded"
      ],
      "metadata": {
        "id": "CdD1yaSInBo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "terms = sorted(lex.keys())\n",
        "\n",
        "front_coded_terms = front_coding_encode(terms)\n",
        "\n",
        "# Extract metadata arrays aligned with sorted terms\n",
        "term_ids = [lex[t][0] for t in terms]\n",
        "dfs = [lex[t][1] for t in terms]\n",
        "tfs = [lex[t][2] for t in terms]\n",
        "\n",
        "# Delta encode metadata\n",
        "term_ids_delta = delta_encode(term_ids)\n",
        "dfs_delta = delta_encode(dfs)\n",
        "tfs_delta = delta_encode(tfs)"
      ],
      "metadata": {
        "id": "9jdL5VVfoRnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "cctx = zstd.ZstdCompressor(level=10)\n",
        "with open('lexicon_comp.pickle.zst', 'wb') as f:\n",
        "  with cctx.stream_writer(f) as compressor:\n",
        "    pickle.dump({\n",
        "          \"front_coded_terms\": front_coded_terms,\n",
        "          \"term_ids\": term_ids,\n",
        "          \"dfs_delta\": dfs_delta,\n",
        "          \"tfs_delta\": tfs_delta,\n",
        "      }, compressor)"
      ],
      "metadata": {
        "id": "YEgMVQEloKfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dctx = zstd.ZstdDecompressor()\n",
        "with open(\"lexicon.pkl.zst\", \"rb\") as f:\n",
        "    with dctx.stream_reader(f) as reader:\n",
        "        lexicon = pickle.load(reader)"
      ],
      "metadata": {
        "id": "y3GF8nehzeyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inverted index"
      ],
      "metadata": {
        "id": "umTng9ATRKwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InvertedIndex:\n",
        "    \"\"\"\n",
        "    A simple inverted index class. Stores term-document mappings for fast retrieval.\n",
        "\n",
        "    Attributes:\n",
        "        lexicon (dict): Maps a token to [termID, docFreq, totalTermFreq].\n",
        "        inv (dict): Contains 'docids' and 'freqs' lists, indexed by termID.\n",
        "        doc (list): Each element is (doc_id, doc_length).\n",
        "        stat (dict): Index statistics (e.g., num_docs, num_terms, num_tokens).\n",
        "\n",
        "    Methods:\n",
        "        num_docs() -> int\n",
        "            Returns the total number of indexed documents.\n",
        "        get_posting(termid: int) -> PostingListIterator\n",
        "            Returns a posting list iterator for the given termID.\n",
        "        get_termids(tokens: list[str]) -> list[int]\n",
        "            Converts tokens to termIDs if found in the lexicon.\n",
        "        get_postings(termids: list[int]) -> list[PostingListIterator]\n",
        "            Returns posting list iterators for each termID.\n",
        "    \"\"\"\n",
        "\n",
        "    class PostingListIterator:\n",
        "        \"\"\"\n",
        "        (Inner class) Iterates over the posting list for a single termID.\n",
        "\n",
        "        Attributes:\n",
        "            docids (list[int]): Document IDs containing this term.\n",
        "            freqs (list[int]): Term frequencies in the corresponding docID.\n",
        "            pos (int): Current index in the posting list.\n",
        "            doc (list): Reference to the main document index.\n",
        "\n",
        "        Methods:\n",
        "            docid() -> int or math.inf\n",
        "                Returns the current docID or math.inf if finished.\n",
        "            score() -> float or math.inf\n",
        "                Returns freq / doc_length or math.inf if finished.\n",
        "            next(target: int = None) -> None\n",
        "                Moves forward or jumps to target docID if specified.\n",
        "            is_end_list() -> bool\n",
        "                Checks if the iterator has reached the end.\n",
        "            len() -> int\n",
        "                Returns the total number of docIDs for this term.\n",
        "        \"\"\"\n",
        "\n",
        "        def __init__(self, docids, freqs, doc):\n",
        "            \"\"\"\n",
        "            Initialize the iterator with document IDs, frequencies, and a reference to the document index.\n",
        "            \"\"\"\n",
        "            self.docids = docids            # List of document IDs where the term appears\n",
        "            self.freqs = freqs              # List of term frequencies corresponding to each document ID\n",
        "            self.pos = 0                    # Start position in the posting list\n",
        "            self.doc = doc                  # Reference to the main document index\n",
        "\n",
        "        def docid(self):\n",
        "            \"\"\"\n",
        "            Returns the current document ID or math.inf if the end of the list is reached.\n",
        "            \"\"\"\n",
        "            if self.is_end_list():\n",
        "                return math.inf\n",
        "            return self.docids[self.pos]\n",
        "\n",
        "        def score(self):\n",
        "            \"\"\"\n",
        "            Computes the term frequency normalized by the document length for the current position.\n",
        "            Returns math.inf if the end of the list is reached.\n",
        "            \"\"\"\n",
        "            if self.is_end_list():\n",
        "                return math.inf\n",
        "            return self.freqs[self.pos]/self.doc[self.docid()][1]\n",
        "\n",
        "        def next(self, target = None):\n",
        "            \"\"\"\n",
        "            Advances to the next position in the posting list or jumps to the target document ID.\n",
        "            \"\"\"\n",
        "            if not target:                              # If no target is specified, move to the next position\n",
        "                if not self.is_end_list():\n",
        "                    self.pos += 1\n",
        "            else:\n",
        "                if target > self.docid():               # If a target is specified, jump to its position if it exists\n",
        "                    try:\n",
        "                        self.pos = self.docids.index(target, self.pos)\n",
        "                    except ValueError:\n",
        "                        self.pos = len(self.docids)     # Move to the end if the target is not found\n",
        "\n",
        "        def is_end_list(self):\n",
        "            \"\"\"\n",
        "            Checks if the iterator has reached the end of the posting list.\n",
        "            \"\"\"\n",
        "            return self.pos == len(self.docids)\n",
        "\n",
        "\n",
        "        def len(self):\n",
        "            \"\"\"\n",
        "            Returns the total number of document IDs in the posting list.\n",
        "            \"\"\"\n",
        "            return len(self.docids)\n",
        "\n",
        "\n",
        "    def __init__(self, lex, inv, doc, stats):\n",
        "        \"\"\"\n",
        "        Initialize the inverted index with its components: lexicon, inverted file, document index, and stats.\n",
        "        \"\"\"\n",
        "        self.lexicon = lex          # Lexicon mapping tokens to [termID, docFreq, totalTermFreq]\n",
        "        self.inv = inv              # Inverted index with 'docids' and 'freqs'\n",
        "        self.doc = doc              # List of documents with IDs and lengths\n",
        "        self.stat = stats           # Index statistics (e.g., number of documents, terms, tokens)\n",
        "\n",
        "    def num_docs(self):\n",
        "        \"\"\"\n",
        "        Returns the total number of indexed documents.\n",
        "        \"\"\"\n",
        "        return self.stats['num_docs']\n",
        "\n",
        "    def get_posting(self, termid):\n",
        "        \"\"\"\n",
        "        Returns a PostingListIterator for the given term ID.\n",
        "        \"\"\"\n",
        "        return InvertedIndex.PostingListIterator(self.inv['docids'][termid], self.inv['freqs'][termid], self.doc)\n",
        "\n",
        "    def get_termids(self, tokens):\n",
        "        \"\"\"\n",
        "        Converts a list of tokens to their corresponding term IDs using the lexicon.\n",
        "        \"\"\"\n",
        "        return [self.lexicon[token][0] for token in tokens if token in self.lexicon]\n",
        "\n",
        "    def get_postings(self, termids):\n",
        "        \"\"\"\n",
        "        Returns a list of PostingListIterators for the given term IDs.\n",
        "        \"\"\"\n",
        "        return [self.get_posting(termid) for termid in termids]\n",
        "\n",
        "inv_index = InvertedIndex(lex, inv, doc, stats)"
      ],
      "metadata": {
        "id": "0tLCJXAPRArI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query processing"
      ],
      "metadata": {
        "id": "RltlC0r_TNtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trec_dl_2020 = ir_datasets.load(\"msmarco-passage/trec-dl-2020\")"
      ],
      "metadata": {
        "id": "tKdmqfpTTPwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TopQueue:\n",
        "    \"\"\"\n",
        "    A simple top-k priority queue to maintain the top-scoring items.\n",
        "\n",
        "    This class uses a min-heap to efficiently store and retrieve the top-k\n",
        "    items based on their scores. Items are tuples of (score, docid).\n",
        "\n",
        "    Attributes:\n",
        "        queue (list[tuple[float, int]]): The priority queue storing (score, docid) pairs.\n",
        "        k (int): The maximum number of items to maintain in the queue.\n",
        "        threshold (float): The minimum score required for an item to enter the queue.\n",
        "\n",
        "    Methods:\n",
        "        size() -> int:\n",
        "            Returns the current number of items in the queue.\n",
        "        would_enter(score: float) -> bool:\n",
        "            Checks if a given score exceeds the threshold and could enter the queue.\n",
        "        clear(new_threshold: float = None) -> None:\n",
        "            Clears the queue and optionally sets a new threshold.\n",
        "        insert(docid: int, score: float) -> bool:\n",
        "            Attempts to insert an item into the queue. Updates the threshold if needed.\n",
        "        __repr__() -> str:\n",
        "            Returns a string representation of the queue.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, k=10, threshold=0.0):\n",
        "        \"\"\"\n",
        "        Initializes the TopQueue with a maximum size and an optional threshold.\n",
        "        \"\"\"\n",
        "        self.queue = []                     # Initialize an empty priority queue (min-heap)\n",
        "        self.k = k                          # Maximum number of items to store\n",
        "        self.threshold = threshold          # Initial score threshold\n",
        "\n",
        "    def size(self):\n",
        "        \"\"\"\n",
        "        Returns the current number of items in the queue.\n",
        "        \"\"\"\n",
        "        return len(self.queue)\n",
        "\n",
        "    def would_enter(self, score):\n",
        "        \"\"\"\n",
        "        Checks if a given score exceeds the current threshold and could enter the queue.\n",
        "        \"\"\"\n",
        "        return score > self.threshold\n",
        "\n",
        "    def clear(self, new_threshold=None):\n",
        "        \"\"\"\n",
        "        Clears all items from the queue and optionally sets a new threshold.\n",
        "        \"\"\"\n",
        "        self.queue = []                     # Empty the queue\n",
        "        if new_threshold is not None:\n",
        "            self.threshold = new_threshold  # Update the threshold if provided\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        Returns a string representation of the queue.\n",
        "        \"\"\"\n",
        "        return f'<{self.size()} items, th={self.threshold} {self.queue}>'\n",
        "\n",
        "    def insert(self, score, query_id, docid):\n",
        "        # Inserts an item into the priority queue if it meets the threshold\n",
        "        if not self.would_enter(score):\n",
        "            return False  # The score is too low to be added to the heap\n",
        "\n",
        "        # Adds the query ID along with the score and document ID\n",
        "        if self.size() < self.k:\n",
        "            heapq.heappush(self.queue, (score, query_id, docid))\n",
        "        else:\n",
        "            heapq.heapreplace(self.queue, (score, query_id, docid))\n",
        "\n",
        "        # Updates the threshold only if the queue contains at least `k` elements\n",
        "        if self.size() >= self.k:\n",
        "            self.threshold = self.queue[0][0]\n",
        "\n",
        "        return True\n",
        "\n",
        "    def sort_descending(self):\n",
        "        # Sorts the queue by score in descending order\n",
        "        self.queue = sorted(self.queue, key=lambda x: x[0], reverse=True)\n",
        "        return self.queue"
      ],
      "metadata": {
        "id": "raah4RBpUOjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BM25"
      ],
      "metadata": {
        "id": "5TFeiv56VWWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LOG_E_OF_2 = math.log(2)        # Natural logarithm of 2 for base conversion\n",
        "LOG_2_OF_E = 1 / LOG_E_OF_2     # Conversion factor for log base-e to base-2\n",
        "\n",
        "# Compute average document length and total number of documents from the index\n",
        "avg_dl = inv_index.stat['num_tokens'] / inv_index.stat['num_docs']\n",
        "N = inv_index.stat['num_docs']\n",
        "\n",
        "def bm25(tf, df, dl, k1=1.2, b=0.75):\n",
        "    \"\"\"\n",
        "    Compute the BM25 relevance score for a term in a document.\n",
        "\n",
        "    Args:\n",
        "        tf (int):   Term frequency, the count of the term in the document.\n",
        "        df (int):   Document frequency, the number of documents containing the term.\n",
        "        dl (float): Document length, the number of tokens in the document.\n",
        "        k1 (float): Parameter controlling term frequency saturation.\n",
        "        b (float):  Parameter controlling document length normalization.\n",
        "\n",
        "    Returns:\n",
        "        float: The BM25 score for the term in the document with respect to the query.\n",
        "    \"\"\"\n",
        "    idf = math.log(1 + (N - df + 0.5) / (df + 0.5)) * LOG_2_OF_E                # IDF weighting\n",
        "    K = k1 * ((1 - b) + b * (dl / avg_dl))                                      # Document length adjustment\n",
        "    term_frequency_component = ((k1 + 1) * tf) / (K + tf)                       # TF component\n",
        "    return idf * term_frequency_component"
      ],
      "metadata": {
        "id": "o8xnoj5oVd4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DAAT BM25"
      ],
      "metadata": {
        "id": "fU01Jd1mVsA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Precompute document lengths\n",
        "doc_lengths = defaultdict(int)\n",
        "for docid, doc_len in inv_index.doc:\n",
        "    doc_lengths[docid] = doc_len\n",
        "\n",
        "def min_docid(postings):\n",
        "    \"\"\"\n",
        "    Find the smallest document ID among active posting list iterators.\n",
        "\n",
        "    Args:\n",
        "        postings (list[PostingListIterator]): Posting list iterators.\n",
        "\n",
        "    Returns:\n",
        "        int: The smallest document ID or math.inf if all lists are exhausted.\n",
        "    \"\"\"\n",
        "\n",
        "    min_docid = math.inf\n",
        "    for p in postings:\n",
        "        if not p.is_end_list():     # Skip completed lists\n",
        "            min_docid = min(p.docid(), min_docid)\n",
        "    return min_docid\n",
        "\n",
        "def daat_bm25(postings, k=10):\n",
        "    \"\"\"\n",
        "    Perform Document-At-A-Time (DAAT) retrieval with BM25 scoring.\n",
        "\n",
        "    Args:\n",
        "        postings (list[PostingListIterator]): Posting lists for terms.\n",
        "        k (int): Number of top results to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        list[tuple[int, float]]: Top-k (docid, score) pairs sorted by score.\n",
        "    \"\"\"\n",
        "\n",
        "    top = TopQueue(k)                               # Initialize top-k priority queue\n",
        "    current_docid = min_docid(postings)             # Start with the smallest document ID\n",
        "\n",
        "    while current_docid != math.inf:                # Process documents until all posting lists are exhausted\n",
        "        score = 0\n",
        "        next_docid = math.inf\n",
        "\n",
        "        for posting in postings:\n",
        "            if posting.docid() == current_docid:    # Check if the term is in the current doc\n",
        "                tf = posting.freqs[posting.pos]\n",
        "                df = posting.len()\n",
        "                dl = doc_lengths[current_docid]\n",
        "\n",
        "                score += bm25(tf, df, dl)\n",
        "\n",
        "                posting.next()                      # Move to the next term occurrence\n",
        "\n",
        "            if not posting.is_end_list():           # Update the smallest doc ID for next iteration\n",
        "                next_docid = min(next_docid, posting.docid())\n",
        "\n",
        "        top.insert(current_docid, score)            # Add the current doc to the top-k queue\n",
        "        current_docid = next_docid                  # Move to the next document\n",
        "\n",
        "    return sorted(top.queue, reverse=True)"
      ],
      "metadata": {
        "id": "FTHwFdb_VvEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TAAT BM25"
      ],
      "metadata": {
        "id": "wfDSymdHVxlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def taat_bm25(postings, k=10):\n",
        "    \"\"\"\n",
        "    Perform Term-At-A-Time (TAAT) retrieval with BM25 scoring.\n",
        "\n",
        "    Args:\n",
        "        postings (list[PostingListIterator]): A list of posting list iterators, one for each query term.\n",
        "        k (int): The maximum number of top documents to retrieve. Default is 10.\n",
        "\n",
        "    Returns:\n",
        "        list[tuple[int, float]]: A sorted list of (docid, score) tuples, ordered by score in descending order.\n",
        "    \"\"\"\n",
        "    A = defaultdict(float)                      # Accumulator for document scores\n",
        "\n",
        "    # Process one term's posting list at a time\n",
        "    for posting in postings:\n",
        "        current_docid = posting.docid()\n",
        "        df = posting.len()                      # Document frequency for the current term\n",
        "\n",
        "        while current_docid != math.inf:\n",
        "            tf = posting.freqs[posting.pos]     # Term frequency in the current document\n",
        "            dl = doc_lengths[current_docid]     # Length of the current document\n",
        "\n",
        "            score = bm25(tf, df, dl)            # Compute BM25 score for the term-document pair\n",
        "            A[current_docid] += score\n",
        "\n",
        "            posting.next()\n",
        "            current_docid = posting.docid()\n",
        "\n",
        "    top = TopQueue(k)\n",
        "\n",
        "    for docid, score in A.items():              # Insert all documents and their scores into the top-k queue\n",
        "        top.insert(docid, score)\n",
        "\n",
        "    return sorted(top.queue, reverse=True)"
      ],
      "metadata": {
        "id": "ldmd_YdvV0rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF"
      ],
      "metadata": {
        "id": "FE0wJ6bvZvR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tfidf_score(tf, df, dl, k1 = 1.2, b = 0.75):\n",
        "    \"\"\"\n",
        "    Compute the TF-IDF score using a normalized term frequency formulation.\n",
        "\n",
        "    Args:\n",
        "        tf (int): Term frequency in the document.\n",
        "        df (int): Document frequency, the number of documents containing the term.\n",
        "        dl (float): Document length, the total number of tokens in the document.\n",
        "        k1 (float): Term frequency saturation parameter.\n",
        "        b (float): Length normalization parameter.\n",
        "\n",
        "    Returns:\n",
        "        float: The TF-IDF score for the term in the document with respect to the query.\n",
        "    \"\"\"\n",
        "    # Compute normalized term frequency\n",
        "    tf_robertson = k1 * tf / (tf + (k1 * ((1 - b) + ((b * dl) / avg_dl))))\n",
        "    # Compute inverse document frequency (IDF) with base-2 logarithm\n",
        "    idf = math.log((N / df) + 1) * LOG_2_OF_E\n",
        "\n",
        "    return tf_robertson * idf"
      ],
      "metadata": {
        "id": "VMSvH3pHZx1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DAAT TF-IDF"
      ],
      "metadata": {
        "id": "60StfjudZzAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def daat_tfidf(postings, k=10):\n",
        "    \"\"\"\n",
        "    Perform Document-At-A-Time (DAAT) retrieval using TF-IDF scoring.\n",
        "\n",
        "    Args:\n",
        "        postings (list[PostingListIterator]): A list of posting list iterators, one for each query term.\n",
        "        k (int): The maximum number of top documents to retrieve. Default is 10.\n",
        "\n",
        "    Returns:\n",
        "        list[tuple[int, float]]: A sorted list of (docid, score) tuples, ordered by score in descending order.\n",
        "    \"\"\"\n",
        "\n",
        "    top = TopQueue(k)                               # Initialize a priority queue for the top-k results\n",
        "    current_docid = min_docid(postings)             # Start with the smallest document ID across postings\n",
        "\n",
        "    while current_docid != math.inf:                # Loop until all documents are processed\n",
        "        score = 0\n",
        "        next_docid = math.inf\n",
        "\n",
        "        for posting in postings:\n",
        "            if posting.docid() == current_docid:    # Check if the term appears in the current document\n",
        "                tf = posting.freqs[posting.pos]\n",
        "                df = posting.len()\n",
        "                dl = doc_lengths[current_docid]\n",
        "\n",
        "                score += tfidf_score(tf, df, dl)    # Accumulate the TF-IDF score for this document\n",
        "\n",
        "                posting.next()\n",
        "\n",
        "            if not posting.is_end_list():           # Update the next smallest document ID\n",
        "                next_docid = min(next_docid, posting.docid())\n",
        "\n",
        "        top.insert(current_docid, score)            # Insert the document and its score into the top-k queue\n",
        "        current_docid = next_docid                  # Move to the next document to be scored\n",
        "\n",
        "    return sorted(top.queue, reverse=True)"
      ],
      "metadata": {
        "id": "g4rIXu5xZ3sL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TAAT TF-IDF"
      ],
      "metadata": {
        "id": "9ShqFmqeZ-j-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def taat_tfidf(postings, k=10):\n",
        "    \"\"\"\n",
        "    Perform Term-At-A-Time (TAAT) retrieval using TF-IDF scoring.\n",
        "\n",
        "    Args:\n",
        "        postings (list[PostingListIterator]): A list of posting list iterators, one for each query term.\n",
        "        k (int): The maximum number of top documents to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        list[tuple[int, float]]: A sorted list of (docid, score) tuples, ordered by score in descending order.\n",
        "    \"\"\"\n",
        "    A = defaultdict(float)                      # Accumulator for document scores\n",
        "\n",
        "    # Process one term's posting list at a time\n",
        "    for posting in postings:\n",
        "        current_docid = posting.docid()\n",
        "\n",
        "        df = posting.len()\n",
        "\n",
        "        while current_docid != math.inf:\n",
        "            tf = posting.freqs[posting.pos]\n",
        "            dl = doc_lengths[current_docid]\n",
        "\n",
        "            score = tfidf_score(tf, df, dl)      # Compute TF-IDF score\n",
        "            A[current_docid] += score\n",
        "\n",
        "            posting.next()\n",
        "            current_docid = posting.docid()\n",
        "\n",
        "    top = TopQueue(k)\n",
        "\n",
        "    for docid, score in A.items():              # Insert all documents and their scores into the top-k queue\n",
        "        top.insert(docid, score)\n",
        "\n",
        "    return sorted(top.queue, reverse=True)"
      ],
      "metadata": {
        "id": "4aKCLnERaE9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "cb-suellcC9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@profile\n",
        "def query_processing(queries_iter, fn):\n",
        "    \"\"\"\n",
        "    Process a list of queries using a specified scoring function.\n",
        "\n",
        "    Args:\n",
        "        queries_iter (iterable): An iterable of query objects.\n",
        "        fn (callable): A scoring function that takes a list of posting list iterators\n",
        "            and returns a list of (docid, score) tuples.\n",
        "\n",
        "    Returns:\n",
        "        list[dict]: A list of results, each containing:\n",
        "            - `query_id` (int): The ID of the processed query.\n",
        "            - `scores` (list[tuple[int, float]]): The list of (docid, score) tuples for the query.\n",
        "    \"\"\"\n",
        "\n",
        "    res = []                                # Store the results for each query\n",
        "\n",
        "    for q in queries_iter:\n",
        "        query = preprocess(q.text)                  # Preprocess the query text\n",
        "        termids = inv_index.get_termids(query)      # Map query tokens to term IDs\n",
        "        postings = inv_index.get_postings(termids)  # Retrieve posting lists for the term IDs\n",
        "\n",
        "        # Compute scores using the provided scoring function and store the result\n",
        "        res.append({'query_id': q.query_id, 'scores': fn(postings)})\n",
        "\n",
        "    return res"
      ],
      "metadata": {
        "id": "Iq1JPosUcE6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(query_processing(trec_dl_2020.queries_iter(), daat_bm25))"
      ],
      "metadata": {
        "id": "_bSUsUYPcHHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bm25_results = query_processing(trec_dl_2020.queries_iter(), taat_bm25)\n",
        "print(bm25_results)"
      ],
      "metadata": {
        "id": "D43GtcFrcehZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(query_processing(trec_dl_2020.queries_iter(), daat_tfidf))"
      ],
      "metadata": {
        "id": "kT_Yrg5Ochkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_results = query_processing(trec_dl_2020.queries_iter(), taat_tfidf)\n",
        "print(tfidf_results)"
      ],
      "metadata": {
        "id": "-5k7M1GLcj3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation with TREC measures"
      ],
      "metadata": {
        "id": "9cDuGBLqcrFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for query in list(trec_dl_2020.queries_iter())[:3]:\n",
        "    print(query)"
      ],
      "metadata": {
        "id": "sqPQjPXwcmN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for qrel in list(trec_dl_2020.qrels_iter())[:3]:\n",
        "  print(qrel)"
      ],
      "metadata": {
        "id": "gue4vZF-cydi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run file generation"
      ],
      "metadata": {
        "id": "mn6MX_S9c_Wy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_run(results):\n",
        "    \"\"\"\n",
        "    Generate a TREC-formatted run list from query results.\n",
        "\n",
        "    Args:\n",
        "        results (list[dict]): A list of query results, where each result contains:\n",
        "            - `query_id` (int): The ID of the query.\n",
        "            - `scores` (list[tuple[float, int]]): A list of (score, doc_id) tuples.\n",
        "\n",
        "    Returns:\n",
        "        list[str]: A list of strings formatted in TREC run format.\n",
        "    \"\"\"\n",
        "\n",
        "    trec_run_list = []                      # List to store TREC-formatted lines\n",
        "\n",
        "    for doc_scores in results:              # Iterate over each query result\n",
        "        rank = 1\n",
        "        query_id = doc_scores['query_id']\n",
        "        scores = doc_scores['scores']\n",
        "\n",
        "        for score, doc_id in scores:\n",
        "            # Format the result as a TREC-compliant line\n",
        "            line = f\"{query_id} Q0 {doc_id} {rank} {score} GOODFELLAS\"\n",
        "            trec_run_list.append(line)\n",
        "            rank += 1\n",
        "\n",
        "    return trec_run_list\n",
        "\n",
        "# Generate TREC-formatted run lists for BM25 and TF-IDF results\n",
        "trec_bm25_run_list = generate_run(bm25_results)\n",
        "trec_tfidf_run_list = generate_run(tfidf_results)\n",
        "\n",
        "# Write the BM25 run list to a TREC-eval compatible file\n",
        "with open(\"trec_eval_bm25_run_file.txt\", \"w\") as f:\n",
        "    for line in trec_bm25_run_list:\n",
        "        f.write(line + \"\\n\")\n",
        "\n",
        "# Write the TF-IDF run list to a separate TREC-eval compatible file\n",
        "with open(\"trec_eval_tfidf_run_file.txt\", \"w\") as f:\n",
        "    for line in trec_tfidf_run_list:\n",
        "        f.write(line + \"\\n\")"
      ],
      "metadata": {
        "id": "2lWGCTFXc0Y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Qrels file generation"
      ],
      "metadata": {
        "id": "G8ZdINIrdFWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qrels_file = []     # List to store lines formatted for TREC-Eval qrels\n",
        "\n",
        "# Iterate over qrels data provided by the TREC DL 2020 dataset\n",
        "for qrel in trec_dl_2020.qrels_iter():\n",
        "    # Format the qrel information as per TREC-Eval requirements\n",
        "    # Format: <query_id> 0 <doc_id> <relevance>\n",
        "    line = f\"{qrel.query_id} 0 {qrel.doc_id} {qrel.relevance}\"\n",
        "    qrels_file.append(line)\n",
        "\n",
        "# Write the qrels list to a file in TREC-Eval compatible format\n",
        "with open(\"trec_eval_qrels_file.txt\", \"w\") as f:\n",
        "    for line in qrels_file:\n",
        "        f.write(line + \"\\n\")"
      ],
      "metadata": {
        "id": "CecgRtdydHyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results"
      ],
      "metadata": {
        "id": "7AoewlVodNCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define evaluation measures for the retrieval models\n",
        "measures = [\n",
        "    P@5,              # Precision at rank 5\n",
        "    P(rel=2)@5,       # Precision at rank 5, considering relevance level >= 2\n",
        "    nDCG@10,          # Normalized Discounted Cumulative Gain at rank 10\n",
        "    AP,               # Average Precision\n",
        "    AP(rel=2),        # Average Precision, considering relevance level >= 2\n",
        "    Bpref,            # Binary preference\n",
        "    Bpref(rel=2),     # Binary preference, considering relevance level >= 2\n",
        "    Judged@10         # Fraction of top 10 documents that were judged\n",
        "]\n",
        "\n",
        "# Load qrels (ground truth relevance judgments)\n",
        "qrels = ir_measures.read_trec_qrels('trec_eval_qrels_file.txt')\n",
        "\n",
        "# Evaluate BM25 results using the defined measures\n",
        "bm25_run = ir_measures.read_trec_run('trec_eval_bm25_run_file.txt')\n",
        "bm25_results = ir_measures.calc_aggregate(measures, qrels, bm25_run)\n",
        "\n",
        "qrels = ir_measures.read_trec_qrels('trec_eval_qrels_file.txt')\n",
        "\n",
        "# Evaluate TF-IDF results using the same qrels and measures\n",
        "tfidf_run = ir_measures.read_trec_run('trec_eval_tfidf_run_file.txt')\n",
        "tfidf_results = ir_measures.calc_aggregate(measures, qrels, tfidf_run)"
      ],
      "metadata": {
        "id": "VKVowLSGdOf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame to compare BM25 and TF-IDF evaluation metrics\n",
        "# Each column represents the results for a retrieval model\n",
        "df = pd.DataFrame({\n",
        "    \"BM25\": bm25_results,\n",
        "    \"TF-IDF\": tfidf_results\n",
        "})\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "id": "iZLOCMdEdW5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison with PyTerrier"
      ],
      "metadata": {
        "id": "7cYHXh2HdbDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyterrier.measures import P, nDCG, AP, Judged\n",
        "\n",
        "# Load the MSMARCO Passage Retrieval dataset\n",
        "dataset = pt.get_dataset('msmarco_passage')\n",
        "\n",
        "# Run an experiment comparing TF-IDF and BM25 retrieval models\n",
        "pt.Experiment(\n",
        "    [\n",
        "        # TF-IDF retriever from the Terrier index\n",
        "        pt.terrier.Retriever.from_dataset('msmarco_passage', 'terrier_stemmed', wmodel='TF_IDF'),\n",
        "        # BM25 retriever from the Terrier index\n",
        "        pt.terrier.Retriever.from_dataset('msmarco_passage', 'terrier_stemmed', wmodel='BM25'),\n",
        "    ],\n",
        "    dataset.get_topics('test-2020'),                                    # Test topics for the experiment\n",
        "    dataset.get_qrels('test-2020'),                                     # Ground truth relevance judgments\n",
        "    eval_metrics=[P@5, P(rel=2)@5, nDCG@10, AP, AP(rel=2), Judged@10],  # Evaluation metrics\n",
        ")"
      ],
      "metadata": {
        "id": "Hl0HQxH9de3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Search Engine"
      ],
      "metadata": {
        "id": "lpW3Bc5Sdj73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_lookup = {doc.doc_id: doc for doc in dataset.docs_iter()}"
      ],
      "metadata": {
        "id": "2XrNaT6Rdp3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SearchEngine:\n",
        "    def __init__(self, index, doc_lookup):\n",
        "        self.index = index\n",
        "        self.doc_lookup = doc_lookup  # Passa la lookup come parametro\n",
        "\n",
        "    def search(self):\n",
        "        user_query = input(\"Enter your search query: \")\n",
        "        tokens = set(preprocess(user_query))  # Pre-elabora la query\n",
        "        term_ids = self.index.get_termids(tokens)  # Ottieni gli ID dei termini\n",
        "        postings = self.index.get_postings(term_ids)  # Ottieni i postings per gli ID dei termini\n",
        "\n",
        "        results = new_taat(user_query, postings, self.index.doc)  # Calcolo del ranking\n",
        "\n",
        "        print(\"Top results:\")\n",
        "        for score, _,doc_id in results:\n",
        "            print(doc_id)\n",
        "            # Recupera i dati del documento dalla lookup\n",
        "            doc_data = self.doc_lookup.get(doc_id)\n",
        "            if doc_data:  # Verifica che il documento esista\n",
        "                print(f\"{doc_data.title} (Score: {score:.2f})\")\n",
        "                print(f\"Link: {doc_data.url}\\n\")\n",
        "            else:\n",
        "                print(f\"Document with ID {doc_id} not found in the dataset.\")\n",
        "\n",
        "\n",
        "# Inizializza il motore di ricerca con la lookup esterna\n",
        "search_engine = SearchEngine(inv_index, doc_lookup)\n",
        "search_engine.search()\n"
      ],
      "metadata": {
        "id": "ZhSKks09dqz5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}